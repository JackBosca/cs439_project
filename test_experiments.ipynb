{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo/Documents/Courses/cs439_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from visualization.segment_visual import *\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device and random seeds for reproducibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    dataset_name = \"wikitext-2-raw-v1\"\n",
    "    model_name = \"distilgpt2\"\n",
    "    max_length = 128\n",
    "    num_workers = 8\n",
    "    pin_memory = True\n",
    "    min_text_length = 50  # Minimum characters for a text to be included\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset\n",
    "def load_and_preprocess_data():\n",
    "    dataset = load_dataset(\"wikitext\", config.dataset_name)\n",
    "    \n",
    "    # Preprocessing function\n",
    "    def preprocess(examples):\n",
    "        texts = [text.strip() for text in examples[\"text\"] \n",
    "                if len(text.strip()) > config.min_text_length \n",
    "                and not text.strip().startswith(\"=\")]\n",
    "        return {\"text\": texts}\n",
    "    \n",
    "    dataset = dataset.map(preprocess, batched=True)\n",
    "    train_texts = dataset[\"train\"][\"text\"][:1000]\n",
    "    val_texts = dataset[\"validation\"][\"text\"][:100]\n",
    "    test_texts = dataset[\"test\"][\"text\"][:100]\n",
    "    \n",
    "    return train_texts, val_texts, test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_preprocess_data():\n",
    "#     # Load TinyStories dataset (small subset for testing)\n",
    "#     dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:5000]\")  # First 5K samples\n",
    "    \n",
    "#     # TinyStories doesn't have validation/test splits by default, so we split manually\n",
    "#     dataset = dataset.train_test_split(test_size=0.2, seed=42)  # 80% train, 20% test/val\n",
    "#     val_test_split = dataset[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "    \n",
    "#     # Preprocessing function (simplified since TinyStories is already clean)\n",
    "#     def preprocess(examples):\n",
    "#         texts = [text.strip() for text in examples[\"text\"] \n",
    "#                 if len(text.strip()) > config.min_text_length]  # No need to filter headers (\"=\")\n",
    "#         return {\"text\": texts}\n",
    "    \n",
    "#     # Apply preprocessing\n",
    "#     train_dataset = dataset[\"train\"].map(preprocess, batched=True)\n",
    "#     val_dataset = val_test_split[\"train\"].map(preprocess, batched=True)  # Validation\n",
    "#     test_dataset = val_test_split[\"test\"].map(preprocess, batched=True)  # Test\n",
    "    \n",
    "#     # Extract texts\n",
    "#     train_texts = train_dataset[\"text\"]\n",
    "#     val_texts = val_dataset[\"text\"]\n",
    "#     test_texts = test_dataset[\"text\"]\n",
    "    \n",
    "#     return train_texts, val_texts, test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer setup\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "# Add padding token if not already present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.texts)\n",
    "        \n",
    "    def sort_by_length(self):\n",
    "        self.texts.sort(key=lambda x: len(x))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        # Create encoding\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return encoding.input_ids.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup\n",
    "def get_model():\n",
    "    model = GPT2LMHeadModel.from_pretrained(config.model_name)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Loss computation\n",
    "\n",
    "def compute_loss(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_items = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            with torch.amp.autocast(device_type=str(device), enabled=True, dtype=torch.float16):\n",
    "                outputs = model(batch, labels=batch)\n",
    "                loss = outputs.loss\n",
    "            total_loss += loss.item() * batch.size(0)  # Sum losses\n",
    "            total_items += batch.size(0)  # Count items\n",
    "    \n",
    "    # Check empty dataloader\n",
    "    if total_items == 0:\n",
    "        return float('nan'), float('nan')  \n",
    "    \n",
    "    avg_loss = total_loss / total_items  # Average loss per item\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()  # Perplexity\n",
    "    \n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, shuffle_mode='random'):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    if shuffle_mode == 'random':\n",
    "        dataloader.dataset.shuffle()\n",
    "    elif shuffle_mode == 'sorted':\n",
    "        dataloader.dataset.sort_by_length()\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.amp.autocast(device_type=str(device), enabled = True, dtype=torch.float16):\n",
    "            outputs = model(batch, labels=batch)\n",
    "        \n",
    "        # Compute loss and backpropagation\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment runner\n",
    "def run_experiment(optimizer_class, lr, weight_decay, batch_size, epochs=2, shuffle_mode='random'):\n",
    "    # Load data\n",
    "    train_texts, val_texts, _ = load_and_preprocess_data()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(train_texts, tokenizer, config.max_length)\n",
    "    val_dataset = TextDataset(val_texts, tokenizer, config.max_length)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = get_model()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, shuffle_mode)\n",
    "        val_loss, val_perplexity = compute_loss(model, val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Perplexity: {val_perplexity:.2f}\")\n",
    "    \n",
    "    \n",
    "\n",
    "    results = {\n",
    "        'train_loss': train_losses[-1],\n",
    "        'val_loss': val_losses[-1],\n",
    "        'val_perplexity': val_perplexity,\n",
    "        'optimizer': optimizer_class.__name__,\n",
    "        'lr': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'shuffle_mode': shuffle_mode,\n",
    "        'train_loss_history': train_losses,\n",
    "        'val_loss_history': val_losses\n",
    "    }\n",
    "    \n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results_list):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for results in results_list:\n",
    "        label = f\"{results['optimizer']}, lr={results['lr']}, bs={results['batch_size']}\"\n",
    "        plt.plot(results['train_loss_history'], label=f\"Train - {label}\")\n",
    "        plt.plot(results['val_loss_history'], '--', label=f\"Val - {label}\")\n",
    "    \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders():\n",
    "    # Load data\n",
    "    train_texts, val_texts, test_texts = load_and_preprocess_data()\n",
    "    \n",
    "    # Create datasets\n",
    "    val_dataset = TextDataset(val_texts, tokenizer, config.max_length)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "\n",
    "    train_dataset = TextDataset(train_texts, tokenizer, config.max_length)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment runner\n",
    "def plot_experiment(model_sb, model_lb, val_loader):\n",
    "    \n",
    "    # Initialize models\n",
    "    nn = 100\n",
    "    visual_Segment(model_sb, model_lb, nn, compute_loss, val_loader)\n",
    "\n",
    "    a, b = generate_filter_normalized_vectors(model_sb)\n",
    "    visual_2D(model_sb, a, b, compute_loss, val_loader, n=4)\n",
    "\n",
    "    c, d = generate_filter_normalized_vectors(model_lb)\n",
    "    visual_2D(model_lb, c, d, compute_loss, val_loader, n=4)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharpness import *\n",
    "def calc_sharpenss(model, train_loader, val_loader):\n",
    "\n",
    "    hv_norm, v = power_iteration_hessian(model, train_loader, device)\n",
    "    sharpenss, _ = compute_epsilon_hessian_sharpness(model, train_loader, None, v)\n",
    "    return sharpenss, hv_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with SGD, lr=0.01, bs=32, shuffle=random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Training:   0%|          | 0/32 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "                                                        \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 5.79 GiB of which 737.81 MiB is free. Including non-PyTorch memory, this process has 5.06 GiB memory in use. Of the allocated memory 4.08 GiB is allocated by PyTorch, and 858.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     49\u001b[39m     plot_experiment(models_list[\u001b[32m0\u001b[39m], models_list[\u001b[32m1\u001b[39m])\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRunning experiment with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp[\u001b[33m'\u001b[39m\u001b[33moptimizer_class\u001b[39m\u001b[33m'\u001b[39m].\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp[\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, bs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, shuffle=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp[\u001b[33m'\u001b[39m\u001b[33mshuffle_mode\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     results, model = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moptimizer_class\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mshuffle_mode\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     results_list.append(results)\n\u001b[32m     28\u001b[39m     models_list.append(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(optimizer_class, lr, weight_decay, batch_size, epochs, shuffle_mode)\u001b[39m\n\u001b[32m     32\u001b[39m val_losses = []\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     val_loss, val_perplexity = compute_loss(model, val_loader)\n\u001b[32m     38\u001b[39m     train_losses.append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, shuffle_mode)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Compute loss and backpropagation\u001b[39;00m\n\u001b[32m     20\u001b[39m loss = outputs.loss\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m optimizer.step()\n\u001b[32m     24\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Courses/cs439_project/.venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Courses/cs439_project/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Courses/cs439_project/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 786.00 MiB. GPU 0 has a total capacity of 5.79 GiB of which 737.81 MiB is free. Including non-PyTorch memory, this process has 5.06 GiB memory in use. Of the allocated memory 4.08 GiB is allocated by PyTorch, and 858.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Main experiment\n",
    "def main():\n",
    "    \n",
    "    # Define experiments to run\n",
    "    torch.cuda.empty_cache()\n",
    "    experiments = [\n",
    "        #{'optimizer_class': torch.optim.SGD, 'lr': 0.01, 'batch_size': 8, 'shuffle_mode': 'random', 'weight_decay': 0.0005},\n",
    "        {'optimizer_class': torch.optim.SGD,    'lr': 0.01, 'batch_size': 32, 'shuffle_mode': 'random', 'weight_decay': 0.0005},\n",
    "        # {'optimizer_class': torch.optim.SGD, 'lr': 0.01, 'batch_size': 8, 'shuffle_mode': 'sorted'},\n",
    "        # {'optimizer_class': torch.optim.Adam, 'lr': 0.001, 'batch_size': 8, 'shuffle_mode': 'random'},\n",
    "        # {'optimizer_class': torch.optim.Adam, 'lr': 0.001, 'batch_size': 8, 'shuffle_mode': 'sorted'},\n",
    "        # {'optimizer_class': torch.optim.AdamW, 'lr': 0.0001, 'batch_size': 8, 'shuffle_mode': 'random'},\n",
    "    ]\n",
    "    \n",
    "    results_list = []\n",
    "    models_list = []\n",
    "    \n",
    "    for exp in experiments:\n",
    "        print(f\"\\nRunning experiment with {exp['optimizer_class'].__name__}, lr={exp['lr']}, bs={exp['batch_size']}, shuffle={exp['shuffle_mode']}\")\n",
    "        results, model = run_experiment(\n",
    "            optimizer_class=exp['optimizer_class'],\n",
    "            lr=exp['lr'],\n",
    "            weight_decay=exp['weight_decay'],\n",
    "            batch_size=exp['batch_size'],\n",
    "            shuffle_mode=exp['shuffle_mode'],\n",
    "        )\n",
    "        results_list.append(results)\n",
    "        models_list.append(model)\n",
    "\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), f\"model_params_{exp['optimizer_class'].__name__}_{exp['lr']}_{exp['batch_size']}.pt\")\n",
    "    # Save results\n",
    "    torch.save(results_list, 'experiment_results.pt')\n",
    "    plot_loss_curves(results_list)\n",
    "    # Print summary\n",
    "    print(\"\\nExperiment Summary:\")\n",
    "    for res in results_list:\n",
    "        print(f\"\\nOptimizer: {res['optimizer']}\")\n",
    "        print(f\"LR: {res['lr']}, Batch Size: {res['batch_size']}, Shuffle: {res['shuffle_mode']}\")\n",
    "        print(f\"Final Train Loss: {res['train_loss']:.4f}, Val Loss: {res['val_loss']:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "models_list = []\n",
    "\n",
    "# Load all parameter files into models_list\n",
    "for file_name in os.listdir():\n",
    "    if file_name.startswith(\"model_params_\") and file_name.endswith(\".pt\"):\n",
    "        print(f\"Loading parameters from {file_name}...\")\n",
    "        state_dict = torch.load(file_name)\n",
    "        models_list.append(state_dict)\n",
    "\n",
    "\n",
    "train_loader, val_loader = create_loaders() \n",
    "\n",
    "plot_experiment(models_list[0], models_list[1], val_loader)\n",
    "for model in models_list:\n",
    "    calc_sharpenss(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
